{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole brain data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import zarr\n",
    "import shutil\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from cellProcessing import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environments\n",
    "\n",
    "* Install `fish_proc` from github https://github.com/zqwei/fish_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 1: specify data and save path\n",
    "dir_root = '/groups/ahrens/ahrenslab/jing/giving_up/20190219/fish2/7dpf-huc_gcamp7ff_gfap_rgeco-GU_slow_fwd-fish02-exp02_20190219_174013/im'\n",
    "# save_root = '/nrs/ahrens/jing/20190219/fish2/7dpf-huc_gcamp7ff_gfap_rgeco-GU_slow_fwd-fish02-exp02_20190219_174013/processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set save folder to Ziqiang's folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_root = './processed'\n",
    "if not os.path.exists(save_root):\n",
    "    os.makedirs(save_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproecssing\n",
    "This will generate the follow files at `save_root` folder\n",
    "* `motion_fix_.h5` -- reference image\n",
    "* `trans_affs.npy` -- affine transform\n",
    "* `Y_2dnorm_ave.h5` -- average image after detrend\n",
    "* `local_pca_data.zarr` -- local pca denoised image (This might be used a x4 file size comparing to the raw data), which can be removed afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "baseline_percentile = 20  \n",
    "baseline_window = 1000   # number of frames\n",
    "numCores = 200  # requests for dask workers\n",
    "preprocessing(dir_root, save_root, numCores=numCores, window=baseline_window, percentile=baseline_percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transpose and rechunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'{save_root}/motion_corrected_data.zarr'):\n",
    "    trans_data_ = zarr.open(f'{save_root}/motion_corrected_data.zarr', mode='r')\n",
    "    trans_data_t = zarr.open(f'{save_root}/motion_corrected_data_by_t.zarr', mode='r')\n",
    "    t, z, x, y = trans_data_.shape\n",
    "    z_, x_, y_, t_ =trans_data_t.shape\n",
    "    if t==t_ and z==z_ and x==x_ and y==y_:\n",
    "        print('Shape of arrays are correct, continue--')\n",
    "    nz = np.random.randint(z)\n",
    "    nt = np.random.randint(t)\n",
    "    if np.array_equal(trans_data_[nt, nz], trans_data_t[nz, :, :, nt]):\n",
    "        shutil.rmtree(f'{save_root}/motion_corrected_data.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.array_equal(trans_data_[nt, nz], trans_data_t[nz, :, :, nt]):\n",
    "    shutil.rmtree(f'{save_root}/motion_corrected_data.zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local PCA\n",
    "* request a single machine with 32 or 48 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_data_t = da.from_zarr(f'{save_root}/motion_corrected_data_by_t.zarr')\n",
    "chunks = trans_data_t.chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask\n",
    "Making a mask for data to decrease the number of computation in demix\n",
    "* remove low intensity pixel\n",
    "* remove low snr pixel\n",
    "* remove low local pixel correlation pixels\n",
    "\n",
    "This will generate the follow files at `save_root` folder\n",
    "* `mask_map.h5` -- mask\n",
    "* `local_correlation_map.h5` -- local correlation\n",
    "* `masked_local_pca_data.zarr` -- df/f computation\n",
    "* `masked_downsampled_local_pca_data.zarr` -- downsampled in time -> used for cell segmentation, which can be removed afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mask parameters\n",
    "intensity_percentile = 40  ## throws out low intensity pixels to create mask: try 20-40\n",
    "dt = 5  # time downsample for cell segmentation\n",
    "mask_brain(save_root, percentile=intensity_percentile, dt=dt, numCores=20, is_skip_snr=True, save_masked_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if mask doesn't look good, choose another value\n",
    "# percentile is applied on a plane by plane basis (i.e. each plane has the different thresholds)\n",
    "mask = File(f'{save_root}/mask_map.h5', 'r')['default'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_ave_ in mask.squeeze():\n",
    "    plt.imshow(n_ave_)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demix\n",
    "This will generate the follow files at `save_root` folder\n",
    "* `demix_rlt` for each block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsplit = 8 # number of split on x and y -- about 256x256 is good\n",
    "demix_cells(save_root, nsplit = nsplit, numCores = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check demix results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_id = (0, 2, 0, 0)\n",
    "check_demix_cells(save_root, block_id, nsplit=8, plot_global=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute df/f\n",
    "One of three choices\n",
    "* df is from NMF components (where background is not included) -- preferred\n",
    "* df is on raw data based on the recomputation of cell F using NMF weights\n",
    "* df/f on pixels without using NMF weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df/f on pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_percentile = 20\n",
    "baseline_window = 1000\n",
    "numCores = 200\n",
    "dff = compute_cell_dff_pixels(dir_root, save_root, \n",
    "                              numCores=numCores, \n",
    "                              window=baseline_window, \n",
    "                              percentile=baseline_percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df/f on raw cell data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cell_dff_raw(dir_root, save_root, \n",
    "                     numCores=numCores, \n",
    "                     window=baseline_window, \n",
    "                     percentile=baseline_percentile, \n",
    "                     nsplit=nsplit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df/f on denoised cell data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cell_dff_NMF(dir_root, save_root, \n",
    "                     numCores=numCores, \n",
    "                     window=baseline_window, \n",
    "                     percentile=baseline_percentile, \n",
    "                     nsplit=nsplit, dt=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
